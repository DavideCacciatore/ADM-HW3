{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b6ad194",
   "metadata": {},
   "source": [
    "# Homework 3 - Algorithmic Methods of Data Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b9eb901",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Package import\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbb5107",
   "metadata": {},
   "source": [
    "# 1. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44b1fa6",
   "metadata": {},
   "source": [
    "#### We start from the list of animes to include in your corpus of documents. In particular, we focus on the top animes ever list. From this list we want to collect the url associated to each anime in the list. The list is long and splitted in many pages. We ask you to retrieve only the urls of the animes listed in the first 400 pages (each page has 50 animes so you will end up with 20000 unique anime urls).\n",
    "\n",
    "#### The output of this step is a .txt file whose single line corresponds to an anime's url."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aee88f5",
   "metadata": {},
   "source": [
    "#### 1.1. Get the list of animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd2daaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls_f(num_of_pages):\n",
    "    #Initialize page zero\n",
    "    num_pages = 0\n",
    "    #Create a list to contain all extracted information\n",
    "    links_of_inter = []\n",
    "\n",
    "    while(num_pages<num_of_pages):\n",
    "        #Connecting to the webpage\n",
    "        URL = f\"https://myanimelist.net/topanime.php?limit={num_pages}\"\n",
    "        page = requests.get(URL)\n",
    "    \n",
    "        #Parsing the page\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "        #Extract all within a certain class\n",
    "        tag_of_inter = soup.find_all('a',{'class':'hoverinfo_trigger fl-l ml12 mr8'})\n",
    "    \n",
    "        #Extract URL of each anime\n",
    "        for e in tag_of_inter:\n",
    "            links_of_inter.append(e.get('href'))\n",
    "    \n",
    "        #Increasing number of pages\n",
    "        num_pages += 50\n",
    "    \n",
    "    return links_of_inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69c2e8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_txt_f(list_to_convert):\n",
    "    #Creating a txt based from the list of URLs\n",
    "    with open(\"url_of_interest.txt\", \"w\") as output:\n",
    "        output.write('\\n'.join(str(line) for line in list_to_convert))\n",
    "    \n",
    "    #Write all list in a single line\n",
    "    #with open(\"url_of_interest.txt\", \"w\") as output:\n",
    "    #    output.write(str(links_of_inter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4155362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrac URLs\n",
    "extracted_urls = extract_urls_f(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78e5fbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the txt file\n",
    "create_txt_f(extracted_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d1e5f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extracted_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee564dd",
   "metadata": {},
   "source": [
    "#### 1.2. Crawl animes\n",
    "\n",
    "Once you get all the urls in the first 400 pages of the list, you:\n",
    "\n",
    "Download the html corresponding to each of the collected urls.\n",
    "After you collect a single page, immediately save its html in a file. In this way, if your program stops, for any reason, you will not lose the data collected up to the stopping point. More details in Important (2).\n",
    "Organize the entire set of downloaded html pages into folders. Each folder will contain the htmls of the animes in page 1, page 2, ... of the list of animes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "317044af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'page_0.html' created\n",
      "Article 0 successfully written!\n",
      "Article 1 successfully written!\n",
      "Article 2 successfully written!\n",
      "Article 3 successfully written!\n",
      "Article 4 successfully written!\n",
      "Article 5 successfully written!\n",
      "Article 6 successfully written!\n",
      "Article 7 successfully written!\n",
      "Article 8 successfully written!\n",
      "Article 9 successfully written!\n",
      "Article 10 successfully written!\n",
      "Article 11 successfully written!\n",
      "Article 12 successfully written!\n",
      "Article 13 successfully written!\n",
      "Article 14 successfully written!\n",
      "Article 15 successfully written!\n",
      "Article 16 successfully written!\n",
      "Article 17 successfully written!\n",
      "Article 18 successfully written!\n",
      "Article 19 successfully written!\n",
      "Article 20 successfully written!\n",
      "Article 21 successfully written!\n",
      "Article 22 successfully written!\n",
      "Article 23 successfully written!\n",
      "Article 24 successfully written!\n",
      "Article 25 successfully written!\n",
      "Article 26 successfully written!\n",
      "Article 27 successfully written!\n",
      "Article 28 successfully written!\n",
      "Article 29 successfully written!\n",
      "Article 30 successfully written!\n",
      "Article 31 successfully written!\n",
      "Article 32 successfully written!\n",
      "Article 33 successfully written!\n",
      "Article 34 successfully written!\n",
      "Article 35 successfully written!\n",
      "Article 36 successfully written!\n",
      "Article 37 successfully written!\n",
      "Article 38 successfully written!\n",
      "Article 39 successfully written!\n",
      "Article 40 successfully written!\n",
      "Article 41 successfully written!\n",
      "Article 42 successfully written!\n",
      "Article 43 successfully written!\n",
      "Article 44 successfully written!\n",
      "Article 45 successfully written!\n",
      "Article 46 successfully written!\n",
      "Article 47 successfully written!\n",
      "Article 48 successfully written!\n",
      "Article 49 successfully written!\n",
      "Directory 'page_1.html' created\n",
      "Article 50 successfully written!\n",
      "Article 51 successfully written!\n",
      "Article 52 successfully written!\n",
      "Article 53 successfully written!\n",
      "Article 54 successfully written!\n",
      "Article 55 successfully written!\n",
      "Article 56 successfully written!\n",
      "Article 57 successfully written!\n",
      "Article 58 successfully written!\n",
      "Article 59 successfully written!\n",
      "Article 60 successfully written!\n",
      "Article 61 successfully written!\n",
      "Article 62 successfully written!\n",
      "Article 63 successfully written!\n",
      "Article 64 successfully written!\n",
      "Article 65 successfully written!\n",
      "Article 66 successfully written!\n",
      "Article 67 successfully written!\n",
      "Article 68 successfully written!\n",
      "Article 69 successfully written!\n",
      "Article 70 successfully written!\n",
      "Article 71 successfully written!\n",
      "Article 72 successfully written!\n",
      "Article 73 successfully written!\n",
      "Article 74 successfully written!\n",
      "Article 75 successfully written!\n",
      "Article 76 successfully written!\n",
      "Article 77 successfully written!\n",
      "Article 78 successfully written!\n",
      "Article 79 successfully written!\n",
      "Article 80 successfully written!\n",
      "Article 81 successfully written!\n",
      "Article 82 successfully written!\n",
      "Article 83 successfully written!\n",
      "Article 84 successfully written!\n",
      "Article 85 successfully written!\n",
      "Article 86 successfully written!\n",
      "Article 87 successfully written!\n",
      "Article 88 successfully written!\n",
      "Article 89 successfully written!\n",
      "Article 90 successfully written!\n",
      "Article 91 successfully written!\n",
      "Article 92 successfully written!\n",
      "Article 93 successfully written!\n",
      "Article 94 successfully written!\n",
      "Article 95 successfully written!\n",
      "Article 96 successfully written!\n",
      "Article 97 successfully written!\n",
      "Article 98 successfully written!\n",
      "Article 99 successfully written!\n",
      "Directory 'page_2.html' created\n",
      "Article 100 successfully written!\n",
      "Article 101 successfully written!\n",
      "Article 102 successfully written!\n",
      "Article 103 successfully written!\n",
      "Article 104 successfully written!\n",
      "Article 105 successfully written!\n",
      "Article 106 successfully written!\n",
      "Article 107 successfully written!\n",
      "Article 108 successfully written!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6a048951adac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;31m#*******HTML EXTRACTION************#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m#Take care a lot of info here!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "principal_wd = os.getcwd()\n",
    "t = 0\n",
    "for i in range(len(extracted_urls)):\n",
    "    if(i%50==0):\n",
    "        #*******FOLDER CREATION************#\n",
    "        page_identifier = i-(49*t)\n",
    "        #Creating a new folder\n",
    "        # Leaf directory\n",
    "        directory = f\"page_{page_identifier}.html\"\n",
    "    \n",
    "        # Parent Directories\n",
    "        parent_dir = principal_wd\n",
    "        \n",
    "        # Path\n",
    "        path = os.path.join(parent_dir, directory)\n",
    "    \n",
    "        # Create the directory\n",
    "        os.makedirs(path)\n",
    "        print(\"Directory '%s' created\" %directory)\n",
    "        \n",
    "        #Change the directory! \n",
    "        os.chdir(path)\n",
    "        \n",
    "        #Increase t by 1\n",
    "        t += 1\n",
    "        #*******FOLDER CREATION************#\n",
    "        \n",
    "        \n",
    "    time.sleep(5)    \n",
    "    #*******HTML EXTRACTION************#\n",
    "    #Take care a lot of info here!!\n",
    "    #Connecting to the webpage\n",
    "    URL = extracted_urls[i]\n",
    "    page = requests.get(URL)\n",
    "    \n",
    "    #Parsing the page\n",
    "    soup_data = BeautifulSoup(page.content, \"html.parser\")\n",
    "    #*******HTML EXTRACTION************#\n",
    "    \n",
    "    #*******SAVE HTML FILE************#\n",
    "    with open(f\"article_{i}.html\", \"w\") as file:\n",
    "        file.write(str(soup_data))\n",
    "    print(f\"Article {i} successfully written!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e65415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
