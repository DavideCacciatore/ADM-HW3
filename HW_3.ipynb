{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b6ad194",
   "metadata": {},
   "source": [
    "# Homework 3 - Algorithmic Methods of Data Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b9eb901",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Package import\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbb5107",
   "metadata": {},
   "source": [
    "# 1. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44b1fa6",
   "metadata": {},
   "source": [
    "#### We start from the list of animes to include in your corpus of documents. In particular, we focus on the top animes ever list. From this list we want to collect the url associated to each anime in the list. The list is long and splitted in many pages. We ask you to retrieve only the urls of the animes listed in the first 400 pages (each page has 50 animes so you will end up with 20000 unique anime urls).\n",
    "\n",
    "#### The output of this step is a .txt file whose single line corresponds to an anime's url."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aee88f5",
   "metadata": {},
   "source": [
    "#### 1.1. Get the list of animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd2daaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls_f(num_of_pages):\n",
    "    #Initialize page zero\n",
    "    num_pages = 0\n",
    "    #Create a list to contain all extracted information\n",
    "    links_of_inter = []\n",
    "\n",
    "    while(num_pages<num_of_pages):\n",
    "        #Connecting to the webpage\n",
    "        URL = f\"https://myanimelist.net/topanime.php?limit={num_pages}\"\n",
    "        page = requests.get(URL)\n",
    "    \n",
    "        #Parsing the page\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "        #Extract all within a certain class\n",
    "        tag_of_inter = soup.find_all('a',{'class':'hoverinfo_trigger fl-l ml12 mr8'})\n",
    "    \n",
    "        #Extract URL of each anime\n",
    "        for e in tag_of_inter:\n",
    "            links_of_inter.append(e.get('href'))\n",
    "    \n",
    "        #Increasing number of pages\n",
    "        num_pages += 50\n",
    "    \n",
    "    return links_of_inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69c2e8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_txt_f(list_to_convert):\n",
    "    #Creating a txt based from the list of URLs\n",
    "    with open(\"url_of_interest.txt\", \"w\") as output:\n",
    "        output.write('\\n'.join(str(line) for line in list_to_convert))\n",
    "    \n",
    "    #Write all list in a single line\n",
    "    #with open(\"url_of_interest.txt\", \"w\") as output:\n",
    "    #    output.write(str(links_of_inter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4155362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrac URLs\n",
    "extracted_urls = extract_urls_f(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e5fbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the txt file\n",
    "create_txt_f(extracted_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1e5f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(extracted_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded1fd3a",
   "metadata": {},
   "source": [
    "##### Read the txt file which we should already have it the cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32402afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the txt file\n",
    "with open('url_of_interest.txt') as f:\n",
    "    extracted_urls = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb7df1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19131"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extracted_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee564dd",
   "metadata": {},
   "source": [
    "#### 1.2. Crawl animes\n",
    "\n",
    "Once you get all the urls in the first 400 pages of the list, you:\n",
    "\n",
    "Download the html corresponding to each of the collected urls.\n",
    "After you collect a single page, immediately save its html in a file. In this way, if your program stops, for any reason, you will not lose the data collected up to the stopping point. More details in Important (2).\n",
    "Organize the entire set of downloaded html pages into folders. Each folder will contain the htmls of the animes in page 1, page 2, ... of the list of animes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317044af",
   "metadata": {},
   "outputs": [],
   "source": [
    "principal_wd = os.getcwd()\n",
    "t = 0\n",
    "\n",
    "starting_point = 0 #This can be changed to select from which URL we will start extracting info\n",
    "ending_point = 6750(extracted_urls) #Take care and modify\n",
    "\n",
    "for i in range(starting_point,ending_point):\n",
    "    if(i%50==0):\n",
    "        #*******FOLDER CREATION************#\n",
    "        page_identifier = i-(49*t)\n",
    "        #Creating a new folder\n",
    "        # Leaf directory\n",
    "        directory = f\"page_{page_identifier}.html\"\n",
    "    \n",
    "        # Parent Directories\n",
    "        parent_dir = principal_wd\n",
    "        \n",
    "        # Path\n",
    "        path = os.path.join(parent_dir, directory)\n",
    "    \n",
    "        # Create the directory\n",
    "        os.makedirs(path)\n",
    "        print(\"Directory '%s' created\" %directory)\n",
    "        \n",
    "        #Change the directory! \n",
    "        os.chdir(path)\n",
    "        \n",
    "        #Increase t by 1\n",
    "        t += 1\n",
    "        #*******FOLDER CREATION************#\n",
    "        \n",
    "        \n",
    "    time.sleep(5)    \n",
    "    #*******HTML EXTRACTION************#\n",
    "    #Take care a lot of info here!!\n",
    "    #Connecting to the webpage\n",
    "    URL = extracted_urls[i]\n",
    "    page = requests.get(URL)\n",
    "    \n",
    "    #Parsing the page\n",
    "    soup_data = BeautifulSoup(page.content, \"html.parser\")\n",
    "    #*******HTML EXTRACTION************#\n",
    "    \n",
    "    #*******SAVE HTML FILE************#\n",
    "    with open(f\"article_{i}.html\", \"w\") as file:\n",
    "        file.write(str(soup_data))\n",
    "    print(f\"Article {i} successfully written!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
