{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b6ad194",
   "metadata": {},
   "source": [
    "# Homework 3 - Algorithmic Methods of Data Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b9eb901",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Package import\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbb5107",
   "metadata": {},
   "source": [
    "# 1. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44b1fa6",
   "metadata": {},
   "source": [
    "#### We start from the list of animes to include in your corpus of documents. In particular, we focus on the top animes ever list. From this list we want to collect the url associated to each anime in the list. The list is long and splitted in many pages. We ask you to retrieve only the urls of the animes listed in the first 400 pages (each page has 50 animes so you will end up with 20000 unique anime urls).\n",
    "\n",
    "#### The output of this step is a .txt file whose single line corresponds to an anime's url."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aee88f5",
   "metadata": {},
   "source": [
    "#### 1.1. Get the list of animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd2daaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls_f(num_of_pages):\n",
    "    #Initialize page zero\n",
    "    num_pages = 0\n",
    "    #Create a list to contain all extracted information\n",
    "    links_of_inter = []\n",
    "\n",
    "    while(num_pages<num_of_pages):\n",
    "        #Connecting to the webpage\n",
    "        URL = f\"https://myanimelist.net/topanime.php?limit={num_pages}\"\n",
    "        page = requests.get(URL)\n",
    "    \n",
    "        #Parsing the page\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "        #Extract all within a certain class\n",
    "        tag_of_inter = soup.find_all('a',{'class':'hoverinfo_trigger fl-l ml12 mr8'})\n",
    "    \n",
    "        #Extract URL of each anime\n",
    "        for e in tag_of_inter:\n",
    "            links_of_inter.append(e.get('href'))\n",
    "    \n",
    "        #Increasing number of pages\n",
    "        num_pages += 50\n",
    "    \n",
    "    return links_of_inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69c2e8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_txt_f(list_to_convert):\n",
    "    #Creating a txt based from the list of URLs\n",
    "    with open(\"url_of_interest.txt\", \"w\") as output:\n",
    "        output.write('\\n'.join(str(line) for line in list_to_convert))\n",
    "    \n",
    "    #Write all list in a single line\n",
    "    #with open(\"url_of_interest.txt\", \"w\") as output:\n",
    "    #    output.write(str(links_of_inter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4155362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrac URLs\n",
    "extracted_urls = extract_urls_f(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e5fbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the txt file\n",
    "create_txt_f(extracted_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1e5f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(extracted_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f92e5d9",
   "metadata": {},
   "source": [
    "##### Read the txt file which we should already have it the cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9584a319",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the txt file\n",
    "with open('url_of_interest.txt', encoding = 'utf8') as f:\n",
    "    extracted_urls = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81035f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19131"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extracted_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee564dd",
   "metadata": {},
   "source": [
    "#### 1.2. Crawl animes\n",
    "\n",
    "Once you get all the urls in the first 400 pages of the list, you:\n",
    "\n",
    "Download the html corresponding to each of the collected urls.\n",
    "After you collect a single page, immediately save its html in a file. In this way, if your program stops, for any reason, you will not lose the data collected up to the stopping point. More details in Important (2).\n",
    "Organize the entire set of downloaded html pages into folders. Each folder will contain the htmls of the animes in page 1, page 2, ... of the list of animes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317044af",
   "metadata": {},
   "outputs": [],
   "source": [
    "principal_wd = os.getcwd()\n",
    "t = 0\n",
    "\n",
    "starting_point = 3258 #This can be changed to select from which URL we will start extracting info\n",
    "ending_point = 6500 #Take care and modify\n",
    "\n",
    "for i in range(starting_point,ending_point):\n",
    "    if(i%50==0):\n",
    "        #*******FOLDER CREATION************#\n",
    "        page_identifier = i-(49*t)\n",
    "        #Creating a new folder\n",
    "        # Leaf directory\n",
    "        directory = f\"page_{page_identifier}.html\"\n",
    "    \n",
    "        # Parent Directories\n",
    "        parent_dir = principal_wd\n",
    "        \n",
    "        # Path\n",
    "        path = os.path.join(parent_dir, directory)\n",
    "    \n",
    "        # Create the directory\n",
    "        os.makedirs(path)\n",
    "        print(\"Directory '%s' created\" %directory)\n",
    "        \n",
    "        #Change the directory! \n",
    "        os.chdir(path)\n",
    "        \n",
    "        #Increase t by 1\n",
    "        t += 1\n",
    "        #*******FOLDER CREATION************#\n",
    "        \n",
    "        \n",
    "    time.sleep(5)    \n",
    "    #*******HTML EXTRACTION************#\n",
    "    #Take care a lot of info here!!\n",
    "    #Connecting to the webpage\n",
    "    URL = extracted_urls[i]\n",
    "    page = requests.get(URL)\n",
    "    \n",
    "    #Parsing the page\n",
    "    soup_data = BeautifulSoup(page.content, \"html.parser\")\n",
    "    #*******HTML EXTRACTION************#\n",
    "    \n",
    "    #*******SAVE HTML FILE************#\n",
    "    with open(f\"article_{i}.html\", \"w\", encoding = 'utf8') as file:\n",
    "        file.write(str(soup_data))\n",
    "    print(f\"Article {i} successfully written!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee51b9d",
   "metadata": {},
   "source": [
    "#### 1.3 Parse downloaded pages\n",
    "\n",
    "At this point, you should have all the html documents about the animes of interest and you can start to extract the animes informations. The list of information we desire for each anime and their format is the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9270cf",
   "metadata": {},
   "source": [
    "* Anime Name (to save as `animeTitle`): String\n",
    "* Anime Type (to save as `animeType`): String\n",
    "* Number of episode (to save as `animeNumEpisode`): Integer\n",
    "* Release and End Dates of anime (to save as `releaseDate` and `endDate`): Convert both release and end date into datetime format.\n",
    "* Number of members (to save as `animeNumMembers`): Integer\n",
    "* Score (to save as `animeScore`): Float\n",
    "* Users (to save as `animeUsers`): Integer\n",
    "* Rank (to save as `animeRank`): Integer\n",
    "* Popularity (to save as `animePopularity`): Integer\n",
    "* Synopsis (to save as `animeDescription`): String\n",
    "* Related Anime (to save as `animeRelated`): Extract all the related animes, but only keep unique values and those that have a hyperlink associated to them. List of strings.\n",
    "* Characters (to save as `animeCharacters`): List of strings.\n",
    "* Voices (to save as `animeVoices`): List of strings\n",
    "* Staff (to save as `animeStaff`): Include the staff name and their responsibility/task in a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "663cc9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def making_several_appends(list_to_append, *element):\n",
    "    list_to_append.extend(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98f93757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coincidence_by_label(soup, label):\n",
    "    return soup.find(\"span\", text=re.compile(label)).next_sibling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5466d47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anime_Title_f(bs,list_name): #change\n",
    "\n",
    "    #l = [None] * 1 #Define the list with length 1\n",
    "    \n",
    "    #Extract all within a certain class\n",
    "    tag_of_inter = bs.find('h1',{'class':'title-name h1_bold_none'})\n",
    "\n",
    "    a = tag_of_inter.text\n",
    "    list_name.append(a) #Change\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d6957fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animeType_f(bs,list_name): \n",
    "    list_name.append(bs.find(text=\"Type:\").findNext('a').contents[0]) \n",
    "    return(bs.find(text=\"Type:\").findNext('a').contents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64ba6449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animeNumEpisode_f(bs,list_name):\n",
    "    list_name.append(int(coincidence_by_label(bs, \"Episodes\").strip()))\n",
    "    return(coincidence_by_label(bs, \"Episodes\").strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f237634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def release_date_f(bs,list_name):\n",
    "    dates = coincidence_by_label(bs, \"Aired\").strip()\n",
    "    release = re.findall(r\".+(?=to)\", str(dates))\n",
    "    end = re.findall(r\"(?<=to).+\", str(dates))\n",
    "    list_name.append(release)\n",
    "    list_name.append(end)\n",
    "    return release,end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4681395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animeNumMembers_f(bs,list_name):\n",
    "    tag_of_inter = bs.find_all('span',{'class':'numbers members'})\n",
    "    p = (re.findall(r\"(?<=<strong>).+(?=</strong>)\", str(tag_of_inter))[0])\n",
    "    if p == 'Unknown':\n",
    "        p = ''\n",
    "    else:\n",
    "        p = int(p.replace(',', ''))\n",
    "    list_name.append(p)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3eaadb33-bcde-4459-a821-2f77dd671495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animeScore_f(bs, list_name):\n",
    "    score = float(bs.find('div', class_ = 'fl-l score').find('div').text)\n",
    "    list_name.append(score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d8d8c63-ed7d-4fff-92fb-671e9497c67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animeUsers_f(bs, list_name):\n",
    "    tag_of_inter = bs.find('div', {'class':'fl-l score'})\n",
    "    users = int(''.join(re.findall(r'\\d+', tag_of_inter.get('data-user'))))\n",
    "    list_name.append(users)\n",
    "    return users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46dc8ea0-8352-4d45-8c82-1232a648f053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animeRank_f(bs, list_name):\n",
    "    tag_of_inter = bs.find_all('span',{'class':'numbers ranked'})\n",
    "    rank = int(re.findall(r\"(?<=<strong>#).+(?=</strong>)\", str(tag_of_inter))[0])\n",
    "    list_name.append(rank)\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30d5f1a7-d1cd-48ce-8a9d-4e8b0817ae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animePopularity_f(bs, list_name):\n",
    "    tag_of_inter = bs.find_all('span',{'class':'numbers popularity'})\n",
    "    pop = int(re.findall(r\"(?<=<strong>#).+(?=</strong>)\", str(tag_of_inter))[0])\n",
    "    list_name.append(pop)\n",
    "    return pop    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e12b2ab-bfd3-4db1-a532-d226c45e15ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animeDescription_f(bs, list_name):\n",
    "    synopsis = bs.find('p', itemprop='description').text\n",
    "    list_name.append(synopsis)\n",
    "    return synopsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "abebda89-79e9-49e7-972b-a67674e5cec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2675906"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### THIS IS A TEST CELL ###\n",
    "# Remember to remove it #\n",
    "os.chdir(\"C:/Users/dcacc/Desktop/File/DAVIDE/Università ROMA/Algorithmic Methods of Data Mining/Html Folders/page_0.html\")\n",
    "html = open(\"article_0.html\", 'r', encoding = 'utf8').read()\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "anime_info = []\n",
    "animeNumMembers_f(bs, anime_info)\n",
    "### THIS IS A TEST CELL ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "47403c93",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'Unknown'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-a39ba7a6387c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0manimeTitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manime_Title_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manime_info\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Keeping variable assignation for debugging purposes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0manimeType\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manimeType_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manime_info\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Keeping variable assignation for debugging purposes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[0manimeNumEpisode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manimeNumEpisode_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manime_info\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Keeping variable assignation for debugging purposes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m             \u001b[0mreleaseDate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mendDate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrelease_date_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manime_info\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Keeping variable assignation for debugging purposes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0manimeNumMembers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manimeNumMembers_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manime_info\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Keeping variable assignation for debugging purposes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-b1d8185306c9>\u001b[0m in \u001b[0;36manimeNumEpisode_f\u001b[1;34m(bs, list_name)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0manimeNumEpisode_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlist_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mlist_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoincidence_by_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Episodes\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoincidence_by_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Episodes\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'Unknown'"
     ]
    }
   ],
   "source": [
    "folder_to_begin = 0 #MUST CUSTOMIZE\n",
    "folder_to_end = 3 #MUST CUSTOMIZE\n",
    "\n",
    "for folder_identifier in range(folder_to_begin, folder_to_end):\n",
    "    #*******THIS MUST BE CUSTOMIZED******\n",
    "    #os.chdir(f\"C:/Users/dcacc/Desktop/File/DAVIDE/Università ROMA/Algorithmic Methods of Data Mining/Html Folders/page_{folder_identifier}.html\")\n",
    "    os.chdir(f\"/Users/brauliovillalobos/Documents/Data Science Master's Degree - Sapienza 2021 2023/ADM - Algorithms of Data Mining/HW_3/Pages_to_extract/page_{folder_identifier}.html\")\n",
    "    #*******THIS MUST BE CUSTOMIZED******\n",
    "    \n",
    "    if folder_identifier == 0:\n",
    "        for anime_identifier in range(50):\n",
    "            #Open the html that is going to be parsed\n",
    "            html = open(f\"article_{anime_identifier}.html\", 'r', encoding = 'utf8').read()\n",
    "    \n",
    "            #Creating a list to consolidate\n",
    "            anime_info = []\n",
    "    \n",
    "            #Parsing the HTML using BS\n",
    "            bs = BeautifulSoup(html) \n",
    "    \n",
    "            #Start calling functions\n",
    "            animeTitle = anime_Title_f(bs, anime_info) #Keeping variable assignation for debugging purposes\n",
    "            animeType = animeType_f(bs, anime_info) #Keeping variable assignation for debugging purposes\n",
    "            animeNumEpisode = animeNumEpisode_f(bs, anime_info) #Keeping variable assignation for debugging purposes\n",
    "            releaseDate, endDate = release_date_f(bs, anime_info) #Keeping variable assignation for debugging purposes\n",
    "            animeNumMembers = animeNumMembers_f(bs, anime_info) #Keeping variable assignation for debugging purposes\n",
    "            animeScore = animeScore_f(bs, anime_info) #Keeping variable assignation for debugging purpose\n",
    "            animeUsers = animeUsers_f(bs, anime_info)\n",
    "            animeRank = animeRank_f(bs, anime_info)\n",
    "            animePopularity = animePopularity_f(bs, anime_info)\n",
    "            animeDescription = animeDescription_f(bs, anime_info)\n",
    "      \n",
    "            #Write a csv\n",
    "            with open(f'anime_{anime_identifier}.tsv', 'w', newline='', encoding = 'utf8') as f_output:\n",
    "                tsv_output = csv.writer(f_output, delimiter='\\t')\n",
    "                tsv_output.writerow(anime_info)\n",
    "    \n",
    "    else:\n",
    "        for anime_identifier in range((folder_identifier)*50, (folder_identifier+1)*50):\n",
    "            #Open the html that is going to be parsed\n",
    "            html = open(f\"article_{anime_identifier}.html\", 'r', encoding = 'utf8').read()\n",
    "    \n",
    "            #Creating a list to consolidate\n",
    "            anime_info = []\n",
    "    \n",
    "            #Parsing the HTML using BS\n",
    "            bs = BeautifulSoup(html) \n",
    "    \n",
    "            #Start calling functions\n",
    "            animeTitle = anime_Title_f(bs, anime_info) #Keeping variable assignation for debugging purposes\n",
    "            animeType = animeType_f(bs, anime_info) #Keeping variable assignation for debugging purposes\n",
    "            animeNumEpisode = animeNumEpisode_f(bs, anime_info) #Keeping variable assignation for debugging purposes\n",
    "            releaseDate,endDate = release_date_f(bs, anime_info) #Keeping variable assignation for debugging purposes\n",
    "            animeNumMembers = animeNumMembers_f(bs, anime_info) #Keeping variable assignation for debugging purposes\n",
    "            animeScore = animeScore_f(bs, anime_info) #Keeping variable assignation for debugging purpose\n",
    "            animeUsers = animeUsers_f(bs, anime_info)\n",
    "            animeRank = animeRank_f(bs, anime_info)\n",
    "            animePopularity = animePopularity_f(bs, anime_info)\n",
    "            animeDescription = animeDescription_f(bs, anime_info)\n",
    "      \n",
    "            #Write a csv\n",
    "            with open(f'anime_{anime_identifier}.tsv', 'w', newline='', encoding = 'utf8') as f_output:\n",
    "                tsv_output = csv.writer(f_output, delimiter='\\t')\n",
    "                tsv_output.writerow(anime_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0b8973",
   "metadata": {},
   "source": [
    "# 2. Search Engine\n",
    "\n",
    "Now, we want to create two different Search Engines that, given as input a query, return the animes that match the query.\n",
    "\n",
    "First, you must pre-process all the information collected for each anime by:\n",
    "\n",
    "Removing stopwords\n",
    "Removing punctuation\n",
    "Stemming\n",
    "Anything else you think it's needed\n",
    "For this purpose, you can use the nltk library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d8d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If need to use nltk for the first time. \n",
    "#import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7599ff43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819aae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_pre_processing_f(text_to_preprocess):\n",
    "    #Import all english stopwords\n",
    "    english_stopwords = stopwords.words('english')\n",
    "    #Removing punctuation\n",
    "    text_to_preprocess.translate(str.maketrans('', '', string.punctuation))\n",
    "    # lowercase the text\n",
    "    text_to_preprocess = text_to_preprocess.lower()\n",
    "    # remove stopwords\n",
    "    preprocessed_text = \"\"\n",
    "    for word in preprocessed_text.split():\n",
    "        if word not in english_stopwords:\n",
    "            preprocessed_text += word + \" \" \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede4fbda",
   "metadata": {},
   "source": [
    "## 2.1 Conjunctive query\n",
    "\n",
    "For the first version of the search engine, we narrow our interest on the Synopsis of each anime. It means that you will evaluate queries only with respect to the anime's description."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8df35b",
   "metadata": {},
   "source": [
    "### 2.1.1 Create your index!\n",
    "\n",
    "Before building the index,\n",
    "\n",
    "Create a file named vocabulary, in the format you prefer, that maps each word to an integer (term_id).\n",
    "Then, the first brick of your homework is to create the Inverted Index. It will be a dictionary of this format:\n",
    "\n",
    "where document_i is the id of a document that contains the word.\n",
    "\n",
    "Hint: Since you do not want to compute the inverted index every time you use the Search Engine, it is worth to think to store it in a separate file and load it in memory when needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa02717",
   "metadata": {},
   "source": [
    "### 2.1.2 Execute the query\n",
    "\n",
    "Given a query, that you let the user enter:\n",
    "\n",
    "What documents do we want?\n",
    "\n",
    "Since we are dealing with conjunctive queries (AND), each of the returned documents should contain all the words in the query. The final output of the query must return, if present, the following information for each of the selected documents:\n",
    "\n",
    "animeTitle\n",
    "animeDescription\n",
    "Url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121a93bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
