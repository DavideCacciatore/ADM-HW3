{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aa29026",
   "metadata": {},
   "source": [
    "# Homework 3 - Algorithmic Methods of Data Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79049b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import ctypes\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import list_urls as l_u\n",
    "import content_html as c_html\n",
    "import anime_information as a_info\n",
    "import vocabularize as voc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defe2611",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "\n",
    "We start from the list of animes to include in our corpus of documents. In particular, we focus on the top animes listed in the first 400 pages. From this list we want to collect the url associated to each anime in the list. The list is long and splitted in many pages. We ask you to retrieve only the urls of the animes listed in the first 400 pages (each page has 50 animes so you will end up with 20000 unique anime urls).\n",
    "The output of this step is a .txt file whose single line corresponds to an anime's url.\n",
    "\n",
    "### 1.1. Get the list of animes\n",
    "We extract from the main pages - the first 400 according to pagination - the urls og animes' web pages that will be retrieved in the next points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b0c327",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "# get the txt file with all the urls\n",
    "l_u.parallelize_extraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23346d4",
   "metadata": {},
   "source": [
    "### 1.2. Get the html files from animes' urls previuosly collected\n",
    "We download - via url from previuos point - and memorize all the html files in a directory organized as follows:\n",
    "- a dir is created for each page, where each html in the main web page is collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2940f8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the html files\n",
    "c_html.get_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a832b443",
   "metadata": {},
   "source": [
    "### 1.3 Get the needed information about animes\n",
    "Here we read each html files previously collected to get the requested information from the web pages and we create tsv files to memorize them. Each directory - so each page - contains a directory where all the tsv of the corresponding html files is memorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d070f303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the info about the animes\n",
    "a_info.parallelize_parsing(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216cc7e3",
   "metadata": {},
   "source": [
    "## 2. Search Engine\n",
    "\n",
    "### 2.1 Conjunctive query\n",
    "\n",
    "#### 2.1.1 Create indeces\n",
    "After collecting the necessary data, we started focusing on the searh engine.\n",
    "We initialized some needed objects and files - JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842f00c3",
   "metadata": {},
   "outputs": [],
   "source": [
    " # initialize object that will be needed: shared \"managed\" dictionaries\n",
    "manager = mp.Manager()\n",
    "vocabulary = manager.dict()\n",
    "inverted_index = manager.dict()\n",
    "complex_index = manager.dict()\n",
    "docs_short = manager.dict()\n",
    "#  shared \"managed\" counter and lock to control tasks incrementing the counter\n",
    "v = manager.Value(ctypes.c_ulonglong, 0)\n",
    "lock = manager.Lock()\n",
    "# stemming utilities\n",
    "nltk.download('punkt')\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8facc08",
   "metadata": {},
   "source": [
    "At this point, we computed some JSON files taht we will use in the next points:\n",
    "\n",
    "    - Vocabulary: a txt file whose lines contain a pair, a pre-processed word and its unique code identifier;\n",
    "    \n",
    "    - Inverted_index : a JSON file whose line contain a pair, a word ID and a list of documents in which the word is present. The documents are represented as a string composed by the string document and their identification number;\n",
    "    \n",
    "    - Tf_complex_index : similar to iverted_index, but the list of documents contains tuples (document, tf index). At this point we only have the tf - we will get the Idf after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9a7243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get JSON files for further calculations\n",
    "voc.parallelize_process_anime(path, vocabulary, inverted_index, complex_index, porter, v, lock)\n",
    "voc.write_index(vocabulary, \"vocabulary\")\n",
    "voc.write_index(inverted_index, \"inverted_index\")\n",
    "voc.write_index(complex_index, \"tf_complex_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cff166",
   "metadata": {},
   "source": [
    "Now we compute two other documents which are needed to get the cosine-similarity: \n",
    "\n",
    "    - TfIdf_complex index : as the tf_complex_index, but with the tfIdf index instead of the tf one. To have two step was necessary since to compute the Idf index some overall information previously lacking were needed. After we got the information (stored in the inverted_index.json), the rest could be done;\n",
    "    \n",
    "    - Docs_short : a complementary json file needed to compute the custom measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6b57e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# at this point you have an incomplete complex_index since the number associated to each doc is only the tf part\n",
    "# get complete tfIdf complex_index\n",
    "voc.get_complex_index(complex_index)\n",
    "voc.write_index(complex_index, \"tfIdf_complex_index\")\n",
    "# get index with documents as key to retrieve information on members/popularity of each anime\n",
    "voc.parallelize_docs_short(path, docs_short, porter)\n",
    "voc.write_docs_short(docs_short)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d68fb0",
   "metadata": {},
   "source": [
    "#### 2.1.2 Execute the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2163b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run conjunctive query and print the result as table\n",
    "docs = voc.conjunctive_query(path, porter)\n",
    "voc.print_tables(docs, path, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a64a17a",
   "metadata": {},
   "source": [
    "### 2.2 Cosine-similarity ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c4210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run cosine similarity query and print the result as table\n",
    "top = voc.cosine_similarity_rank(porter, 5)\n",
    "voc.print_tables(top, path, \"tfIdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e48a18",
   "metadata": {},
   "source": [
    "## 3. Custom ranking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743effc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run custom query and print the result as table\n",
    "top = voc.custom_rank(porter, 5)\n",
    "voc.print_tables(top, path, \"tfIdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
